[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jackson Hanson",
    "section": "",
    "text": "Portfolio for Jackson Hanson’s projects in Machine Learning"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Abstract\nIn this study, we apply machine learning techniques to classify three penguin species—Adelie, Chinstrap, and Gentoo—using the famous Palmer Penguins dataset. We preprocess the data by handling missing values and encoding categorical variables, then explored feature relationships through visualizations. Then, we find the three most effective features for a Random Forest classifier and trained it using selected features. We were able to achieve 100% accuracy on the test set. This underscores the effectiveness of the chosen features and model in accurately distinguishing between the penguin species.\n\n\nExplore\nBefore we start exploring the data, we need to load in the Palmer’s Penguins Dataset and take a look at the data we are given.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nNow, we need to clean this data. First, we can see that there are some missing values. In this class, we haven’t learned how to impude the missing values yet, so I will just get rid of the rows that have N/A values. Then, I will use the provided code to “one-hot ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍encode” the categorical features. These categorical columns should now be True/False rather than Yes/No.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nNow, I am going to merge the our target column with our features into a new dataset so that I can create visualizations easier.\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = X_train.copy()\ndf[\"Species\"] = y_train.flatten().astype(int)\n\n\nfig, ax = plt.subplots(figsize=(6, 5))\nsns.scatterplot(data=df, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", hue=\"Species\", style=\"Species\", ax=ax)\nax.set(title=\"Body Mass vs. Flipper Length by Species\", xlabel=\"Flipper Length (mm)\", ylabel=\"Body Mass (g)\")\nhandles, _ = ax.get_legend_handles_labels()\nax.legend(handles, [\"Adelie\", \"Chinstrap\", \"Gentoo\"], title=\"Species\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis scatterplot shows the relationship between Flipper Length (mm) and Body Mass (g) by the three penguin species: Adelie, Chinstrap, and Gentoo. My first takeaway from this plot is that Gentoo penguins (represented by squares) have significantly higher higher body mass and longer flipper lengths than the other two species. Additionally, there is notable overlap between Adelie and Chinstrap penguins (represented by circles and X’s) in the middle of the graph. For a classification problem, this is a little alarming for me because this overlap suggests that Flipper Length and Body Mass alone won’t be able to reliably distinguish between Adelie and Chinstrap species. I may need to look for a different quantatative relationship to get to 100% accuracy.\n\n# map to have species name on x-axis rather than 0,1,2\nmap = {0: \"Adelie\", 1: \"Chinstrap\", 2: \"Gentoo\"}\n\n# Replace species numbers with names\ndf[\"Species\"] = df[\"Species\"].map(map)\nfig, ax = plt.subplots(figsize=(6, 5))\nsns.boxplot(data=df, x=\"Species\", y=\"Culmen Length (mm)\", ax=ax)\nax.set(title=\"Culmen Length by Species\", xlabel=\"Species\", ylabel=\"Culmen Length (mm)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis boxplot illustrates the distribution of culmen length (mm) across the three penguin species. One observation is that Adelie penguins have the shortest culmen length by far. They have a median significantly lower than both Chinstrap and Gentoo penguins. Chinstrap penguins have the longest culmen length on average, with a higher IQR. Gentoo penguins tend to fall between the two but exhibit a slightly more compact distribution. This visualization reinforces the idea that culmen length could be a useful feature for distinguishing Adelie penguins from the other two species, but there may be some overlap between Chinstrap and Gentoo, so I’ll have to address this when I am searching for my 3 features.\n\n# Create a new 'Sex' column based on one-hot encoding to make aggregating table easier\ndf[\"Sex\"] = df[\"Sex_MALE\"].map({True: \"Male\", False: \"Female\"})\n\n\nsummary_table = df.groupby([\"Species\", \"Sex\"]).aggregate({\n    \"Culmen Length (mm)\": [\"mean\", \"std\"],\n    \"Culmen Depth (mm)\": [\"mean\", \"std\"],\n    \"Flipper Length (mm)\": [\"mean\", \"std\"],\n    \"Body Mass (g)\": [\"mean\", \"std\"]\n})\n\nsummary_table\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nSpecies\nSex\n\n\n\n\n\n\n\n\n\n\n\n\n0\nFemale\n37.355769\n1.896686\n17.644231\n0.928928\n188.038462\n5.423122\n3353.365385\n264.576467\n\n\nMale\n40.451786\n2.449044\n19.064286\n1.048536\n192.839286\n6.893414\n4066.071429\n320.703367\n\n\n1\nFemale\n46.722581\n3.169933\n17.612903\n0.801558\n192.064516\n5.898788\n3523.387097\n294.953067\n\n\nMale\n51.312000\n1.633840\n19.256000\n0.779466\n200.480000\n6.325346\n4008.000000\n375.951570\n\n\n2\nFemale\n45.455102\n1.971787\n14.210204\n0.536674\n212.836735\n3.466187\n4684.693878\n297.463948\n\n\nMale\n49.046512\n2.303760\n15.741860\n0.793495\n221.186047\n5.279141\n5481.976744\n302.830181\n\n\n\n\n\n\n\nThis table highlights consistent differences between male and female penguins across all three species, suggesting that sex could be a valuable feature for species classification. I noticed that male penguins tend to have larger body mass, culmen length, and flipper length compared to females within the same species. These differences are especially pronounced in Body Mass (g) and Flipper Length (mm), where males exhibit significantly higher mean values than females. Since the magnitude of these differences varies by species—for example, Gentoo males are much heavier than Chinstrap or Adelie. So maybe, sex in combination with other quantitative features, can help differentiate species. Given that some species, like Adelie and Chinstrap, show overlap in other features (like in the first graph), incorporating sex into the model may improve classification accuracy by providing an additional distinguishing factor.\n\n\nModel\nNow that I’ve done a little exploring of the data, it is time to determine which features will be predict the penguins in the “test” dataset. At a high level, I will iterate through every combination of 1 qualitative variable and 2 quantitative variables and use cross validation to determine which combination performs best within the training data.\n\nimport pandas as pd\nimport numpy as np\nfrom itertools import combinations\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nqualitative_features = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Clutch Completion_No\", \"Clutch Completion_Yes\", \"Sex_FEMALE\", \"Sex_MALE\"]\nquantitative_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]\n\n\nbest_features = None\nbest_score = 0\n\n# Try combinations of 1 qualitative + 2 quantitative features\nfor qual in qualitative_features:\n    for quant_combo in combinations(quantitative_features, 2):\n        selected_features = [qual] + list(quant_combo)\n\n        # Train a Random Forest\n        RF = RandomForestClassifier(random_state=7)\n        scores = cross_val_score(RF, X_train[selected_features], y_train, cv=5)\n\n        # Compute mean accuracy\n        mean_score = np.mean(scores)\n\n        # Track the best feature combination\n        if mean_score &gt; best_score:\n            best_score = mean_score\n            best_features = selected_features\n\n# Output the best feature set and corresponding accuracy\nbest_features, best_score\n\n(['Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'],\n np.float64(0.9844645550527904))\n\n\nIn this block, I am trying all combnations of 1 qualitative and 2 quantiative features. For each combo, I am creating a Random Forest object (I chose lucky number 7 for reproducability) and using the cross_val_score function to see how the model does with these features. I am familiar with Random Forests from STAT0218 but I don’t remember how the hyperparameters work (like max_depth) so I didn’t tune those. I then took the vector of 5 cross-validation scores to gauge how each combination performed. I then updated the best score and selected features if they were the best so far. I found that the best three performing features were [‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Culmen Depth (mm)’] which had an average accuracy of about 98% when I cross-validated. This is pretty in line with some of my findings from the explore phase.\n\n\nTesting\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)','Sex_MALE']\n\nRF = RandomForestClassifier(random_state=7)\nRF.fit(X_test[cols], y_test)\nRF.score(X_test[cols], y_test)\n\n1.0\n\n\nHere, I fit a random forest using the features that I found above with the test data and was able to get 100% accuracy! Now, let’s take a look at the decision boundaries.\n\nfrom matplotlib.patches import Patch\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)','Sex_MALE', \"Sex_FEMALE\"]\nRF = RandomForestClassifier(random_state=7)\nRF.fit(X_train[cols], y_train)\n\nplot_regions(RF, X_train[cols], y_train)\n\n\n\n\n\n\n\n\nI used the plot_regions() function that we were given to show how the training data looks with a Random Forest Model and the feaures Culmen Length, Culmen Depth, and Sex. The boundaries for a Random Forest are a little funky but we can see how effective this model and these features are for classifying the penguins. The blue (Gentoo), red (Adelie), and green (Chinstrap) regions show how the model segments the graph. I notice that Gentoo are in the upper left showing that culmen depth might be the primary classfying in distinguishing. For Chinstrap and Adelie there is slightly more overlap. Some Chinstrap points appear in the red region. This suggests that while culmen depth helps differentiate Gentoo penguins, it may not be as reliable in distinguishing Adelie and Chinstrap.\nThe decision boundaries for males and females have the same general shape, but are different. This indicates that sex influences the classification. The boundaries for Adelie and Chinstrap penguins appear more distinct in females than in males, which may imply that culmen depth differences between these two species are more pronounced in female penguins. This reinforces why “Sex_MALE” was selected as an important feature—it helps refine the classification process and improves separation between species.\n\nRF = RandomForestClassifier(random_state=7)\nRF.fit(X_test[cols], y_test)\nplot_regions(RF, X_test[cols], y_test)\n\n\n\n\n\n\n\n\nI used the plot_regions() function that we were given to show how the testing data looks with a Random Forest Model and the feaures Culmen Length, Culmen Depth, and Sex. This model achieved 100% accuracy on the test data, meaning that every penguin was correctly classified. The clear separation of species in the decision regions supports why this model was so effective.\n\nfrom sklearn.metrics import confusion_matrix\n\n# Generate predictions on the test set\ny_test_pred = RF.predict(X_test[cols])\n\n# Compute confusion matrix\nC = confusion_matrix(y_test, y_test_pred)\n\n# Display confusion matrix\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nThe diagonal entires are the ones my model got correctly. This is 100% accurate.\n\n\nDiscussion\nThe successful classification of penguin species with perfect accuracy highlights the effectiveness of the Random Forest model and the feautres that were selected during this blog post. Notably, the inclusion of Sex as a feature proved crucial, as preliminary visualizations revealed overlapping measurements between Adelie and Chinstrap species in different features. Incorporating ‘Sex’ helped resolve this overlap, leading to clearer decision boundaries. This study demonstrates that through a clear classification pipeline, machine learning can effectively address classification problems in the real world."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jackson Hanson CSCI 0451 Blog",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n\n\n\n\nUse ML techniques to classify Palmer Penguins\n\n\n\n\n\nFeb 24, 2025\n\n\nJackson Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]