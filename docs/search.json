[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jackson Hanson",
    "section": "",
    "text": "Portfolio for Jackson Hanson’s projects in Machine Learning"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Replication Study",
    "section": "",
    "text": "Abstract\nIn this Blog Post, we will examine racial disparities in healthcare algorithms. We will look at medical expenditures and patient risk scores to try and reproduce what Obermeyer et al. found in their 2019 study. We will use a polynomial Ridge regression to find the cost Black patients incur when compared to White Patients. We will find that on average, Black patients incur 77% of the medical costs of equally sick White patients which aligns with what Obermeyer et al. found. These findings suggest that cost-based risk algorithms may systematically underestimate the healthcare needs of Black patients.\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Go through the columns and save the sames of the columns that end in '_elixhauser_tm1' \nchronic_condition_cols = [col for col in df.columns if col.endswith('_elixhauser_tm1')]\n\n# Add column for the percentile using qcut\ndf['risk_score_percentile'] = pd.qcut(df['risk_score_t'], 100, labels=False)\n\n# Number of chronic conditions for each patient\ndf['num_active_chronic_conditions'] = df[chronic_condition_cols].sum(axis=1)\n\n# Mean number of active chronic conditions WITHIN each risk score percentile\nsummary_df = df.groupby(['risk_score_percentile', 'dem_female', 'race'])[\n    'num_active_chronic_conditions'].mean().reset_index()\n\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\n# Define color palette and marker styles for race:\npalette = {\"black\": \"darkblue\", \"white\": \"green\"}\nfixed_markers = {\"black\": \"o\", \"white\": \"s\"} \n\n# Loop through genders\nfor i, (gender_label, gender_value) in enumerate([(\"Male\", 0), (\"Female\", 1)]):\n    ax = axes[i]\n    subset = summary_df[summary_df[\"dem_female\"] == gender_value]\n\n    # Scatter plot:\n    sns.scatterplot(\n        data=subset,\n        x=\"num_active_chronic_conditions\",  # Now using the correctly calculated mean\n        y=\"risk_score_percentile\",\n        hue=\"race\",\n        style=\"race\",\n        palette=palette,\n        markers=fixed_markers,\n        s=60,\n        ax=ax\n    )\n\n    ax.set_xlabel(\"Mean Number of Chronic Illnesses\")\n    ax.set_title(gender_label)\n    ax.grid(True, linestyle=\"--\", alpha=0.6)\n\n# Set y-label on the left plot only\naxes[0].set_ylabel(\"Percentile Risk Score (from algorithm)\")\n\n# Place a common legend on the right side of the figure\nhandles, labels = axes[1].get_legend_handles_labels()\nfig.legend(handles, labels, title=\"Race\", loc=\"center right\")\n\n# Remove individual legends from each subplot\naxes[0].get_legend().remove()\naxes[1].get_legend().remove()\n\nplt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust layout to fit the legend\nplt.show()\n\n\n\n\n\n\n\n\nThe graph shows that for the same number of chronic illnesses, Black patients (blue circles) tend to receive lower risk scores compared to White patients. Since risk scores are often used to determine eligibility for high-risk care management programs, this suggests that Patient A (Black) is less likely to be referred than Patient B (White), despite having the same health conditions. This gap implies that the algorithm may be underestimating the healthcare needs of Black patients, potentially due to its reliance on healthcare spending patterns rather than direct clinical measures. There looks like there is bias baked in here. As a result, Black patients may face reduced access to critical medical interventions, highlighting a racial bias in the risk assessment process that could contribute to inequities in healthcare treatment and outcomes.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Mean total medical expenditure per percentile of risk score\nrisk_score_summary = df.groupby(['risk_score_percentile', 'race'])['cost_t'].mean().reset_index()\n\n# Mean total medical expenditure per number of chronic conditions\nchronic_condition_summary = df.groupby(['gagne_sum_t', 'race'])['cost_t'].mean().reset_index()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\n\n# Expenditure vs. Risk Score Percentile\nsns.scatterplot(\n    data=risk_score_summary,\n    x=\"risk_score_percentile\",\n    y=\"cost_t\",\n    hue=\"race\",\n    style=\"race\",\n    palette=palette,\n    markers = fixed_markers,\n    alpha=0.6,\n    ax=axes[0]\n)\naxes[0].set_xlabel(\"Percentile Risk Score\")\naxes[0].set_ylabel(\"Total Medical Expenditure\")\naxes[0].set_yscale(\"log\")\naxes[0].set_title(\"Total Medical Expenditure vs. Risk Score Percentile\")\naxes[0].grid(True, linestyle=\"--\", alpha=0.6)\n\n# Expenditure vs. Number of Chronic Illnesses\nsns.scatterplot(\n    data=chronic_condition_summary,\n    x=\"gagne_sum_t\",\n    y=\"cost_t\",\n    hue=\"race\",\n    style=\"race\",\n    palette=palette,\n    markers = fixed_markers,\n    alpha=0.6,\n    ax=axes[1]\n)\naxes[1].set_xlabel(\"Number of Chronic Illnesses\")\naxes[1].set_title(\"Total Medical Expenditure vs. Chronic Illnesses\")\naxes[1].set_yscale(\"log\") \naxes[1].grid(True, linestyle=\"--\", alpha=0.6)\n\n# Adjust legend position\nhandles, labels = axes[1].get_legend_handles_labels()\nfig.legend(handles, labels, title=\"Race\", loc=\"center right\")\n\n# Remove individual legends from subplots\naxes[0].get_legend().remove()\naxes[1].get_legend().remove()\n\n\nplt.show()\n\n\n\n\n\n\n\n\nThe graph reveals relationship between total medical expenditure and both risk score percentile and number of chronic illnesses. Again, it is obvious here that there is a disparity between black vs white. In the left panel, we see that as risk score percentiles increase, total medical expenditure rises. This makes sense, but when you take a look at black paitents, you see that Black patients consistently have lower expenditures than White patients at the same risk score percentile except for a few outliers. Similarly as the earleir plot, this suggests that the algorithm may underestimate the healthcare needs of Black patients or that they receive fewer medical interventions despite similar risk assessments.\nIn the right panel, a similar trend emerges: as the number of chronic illnesses increases, total medical expenditures also increase, but again, Black patients tend to have lower expenditures than White patients with the same number of chronic conditions. The exact cause of this difference could be caused by a few things like differences in healthcare access, treatment patterns, or systemic biases in how medical resources are dispersed.\n\nsubset_df = df[df[\"gagne_sum_t\"] &lt;= 5]\n(len(subset_df) / len(df)) * 100\n\n95.53952115447689\n\n\nOur decision to focus on patients with 5 or fewer chronic conditions is supported by the fact that they make up approximately 96% of the dataset. This ensures that our analysis remains representative of the majority of patients while minimizing the influence of extreme cases with very high chronic condition counts. Additionally, as the number of chronic conditions increases, the number of patients in those categories becomes much smaller, leading to increased variability in medical costs and potential instability in the model.\n\n# Remove patients with $0 medical cost (log(0) is undefined)\ndf = df[df[\"cost_t\"] &gt; 0]\n\n# Apply natural log transformation to medical cost\ndf[\"log_cost_t\"] = np.log(df[\"cost_t\"])\n\ndf[[\"cost_t\", \"log_cost_t\"]].head()\n\n\n\n\n\n\n\n\ncost_t\nlog_cost_t\n\n\n\n\n0\n1200.0\n7.090077\n\n\n1\n2600.0\n7.863267\n\n\n2\n500.0\n6.214608\n\n\n3\n1300.0\n7.170120\n\n\n4\n1100.0\n7.003065\n\n\n\n\n\n\n\n\n# Create a dummy variable for race\ndf[\"race_black\"] = (df[\"race\"] == \"black\").astype(int)\n\n\n# Define predictor variables (X) and target variable (y)\nX = df[[\"race_black\", \"gagne_sum_t\"]]  # Predictors: race and number of chronic illnesses\ny = df[\"log_cost_t\"]  # Target: log-transformed medical cost\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\nimport warnings\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n# Step 2: Function to generate polynomial features efficiently\ndef add_polynomial_features(X, degree):\n    X_ = X.copy()\n    for j in range(1, degree):  # Start at degree 2 for additional features\n        X_[f\"poly_{j}\"] = (X_[\"gagne_sum_t\"] ** j).astype(\"float32\")  # Ensure memory-efficient float32\n    return X_\n\n# Step 3: Loop through polynomial degrees (1 to 11) and regularization strengths (10^k for k = -4 to 4)\ndegrees = range(1, 12)\nalphas = [10 ** k for k in range(-4, 5)]\n\nbest_score = float(\"inf\")\nbest_params = None\n\n# Suppress warnings during model fitting\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\")\n\n    # Grid search over polynomial degrees and regularization strengths with memory optimization\n    for degree in degrees:\n        X_poly = add_polynomial_features(X, degree)  # Add polynomial features\n\n        for alpha in alphas:\n            model = Ridge(alpha=alpha)  # Ridge regression with given alpha\n            scores = cross_val_score(model, X_poly, y, scoring=\"neg_mean_squared_error\", cv=5) \n            mean_score = -np.mean(scores)  # Convert to positive MSE\n\n            if mean_score &lt; best_score:  # Track best combination\n                best_score = mean_score\n                best_params = (degree, alpha)\n\n# Display best parameters found\nprint(f\"Best Polynomial Degree: {best_params[0]}\")\nprint(f\"Best Regularization Strength (alpha): {best_params[1]}\")\nprint(f\"Best Cross-Validated MSE: {best_score:.4f}\")\n\nBest Polynomial Degree: 10\nBest Regularization Strength (alpha): 1\nBest Cross-Validated MSE: 1.5084\n\n\n\nbest_degree = best_params[0]\nbest_alpha = best_params[1]\n\nX_poly = add_polynomial_features(X, best_degree)\nfinal_model = Ridge(alpha=best_alpha)\nfinal_model.fit(X_poly, y)\n\nRidge(alpha=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Ridge?Documentation for RidgeiFittedRidge(alpha=1) \n\n\n\nX_poly.head()\n\n\n\n\n\n\n\n\nrace_black\ngagne_sum_t\npoly_1\npoly_2\npoly_3\npoly_4\npoly_5\npoly_6\npoly_7\npoly_8\npoly_9\n\n\n\n\n0\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0\n3\n3.0\n9.0\n27.0\n81.0\n243.0\n729.0\n2187.0\n6561.0\n19683.0\n\n\n2\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n0\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n0\n1\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\n\n\n\n\n\nBecause race_black is the first column in our training data, we need the first coefficent because it corresponds to race_black.\n\nrace_black_coef = final_model.coef_[0]\nrace_black_coef\n\nnp.float64(-0.26704537754160196)\n\n\n\ncost_ratio = np.exp(race_black_coef) * 100\ncost_ratio\n\nnp.float64(76.56383278920148)\n\n\nI found a cost ratio of 77%. This suggests that on average, Black patients incur only 77% of the medical expenditures of White patients with the same number of chronic illnesses. I would say that this aligns with the results of Obermeyer et al., which identified that Black patients were assigned lower risk scores than White patients with the same level of medical complexity, leading to under-allocation of healthcare resources. This means that in industry, cost-based algorithms are used to determine patient risk, and they may systematically underestimate the healthcare needs of Black patients. This supports Obermeyer et al.’s argument that cost is an inadequate proxy for healthcare need and that such models must be adjusted to account for racial biases.\n\n\nDiscussion\nI conclude that what I replicated from Obermeyer et al demonstrates a racial bias that doesn’t satisfy the sufficiency criterion. Obermeyer et al. found that Black patients had more chronic illnesses than White patients at the same risk score, meaning that the model’s predictions were not equally valid across racial groups. This is a violation of sufficiency because the same predicted risk score does not correspond to the same true healthcare need across Black and White patients. As a result, Black patients are systematically assigned lower risk scores despite having a higher actual burden of illness. This leads to a reduced access to critical healthcare resources, where they only get about 77% of the spending as White patients. This study highlights the dangers of relying on cost-based algorithms for risk assessment."
  },
  {
    "objectID": "posts/new-new-test-post copy/index.html",
    "href": "posts/new-new-test-post copy/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post copy/index.html#math",
    "href": "posts/new-new-test-post copy/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/Auditing-Bias/index.html#how-many-individuals-are-in-the-data",
    "href": "posts/Auditing-Bias/index.html#how-many-individuals-are-in-the-data",
    "title": "Blog Post 2 - Auditing Bias",
    "section": "1. How many individuals are in the data?",
    "text": "1. How many individuals are in the data?\nTo find the number of individual in the data, we can take a look at the number of rows.\n\ndf.shape[0]\n\n214480"
  },
  {
    "objectID": "posts/Auditing-Bias/index.html#of-these-individuals-what-proportion-have-target-label-equal-to-1-in-employment-prediction-these-would-correspond-to-employed-individuals.",
    "href": "posts/Auditing-Bias/index.html#of-these-individuals-what-proportion-have-target-label-equal-to-1-in-employment-prediction-these-would-correspond-to-employed-individuals.",
    "title": "Blog Post 2 - Auditing Bias",
    "section": "2. Of these individuals, what proportion have target label equal to 1? In employment prediction, these would correspond to employed individuals.",
    "text": "2. Of these individuals, what proportion have target label equal to 1? In employment prediction, these would correspond to employed individuals.\nBecause the employment vector is a column vector of booleans, the mean of that column will return the proportion of employed individuals in this dataset.\n\ndf[\"label\"].mean()\n\nnp.float64(0.4513474449832152)"
  },
  {
    "objectID": "posts/Auditing-Bias/index.html#of-these-individuals-how-many-are-in-each-of-the-groups",
    "href": "posts/Auditing-Bias/index.html#of-these-individuals-how-many-are-in-each-of-the-groups",
    "title": "Blog Post 2 - Auditing Bias",
    "section": "3. Of these individuals, how many are in each of the groups?",
    "text": "3. Of these individuals, how many are in each of the groups?\nWe can use the value_counts() method to get the number of people in each racial group.\n\ndf[\"group\"].value_counts()\n\ngroup\n1    165969\n2     20614\n8     10544\n6     10141\n9      5803\n3       836\n5       401\n7       158\n4        14\nName: count, dtype: int64"
  },
  {
    "objectID": "posts/Auditing-Bias/index.html#in-each-group-what-proportion-of-individuals-have-target-label-equal-to-1",
    "href": "posts/Auditing-Bias/index.html#in-each-group-what-proportion-of-individuals-have-target-label-equal-to-1",
    "title": "Blog Post 2 - Auditing Bias",
    "section": "4. In each group, what proportion of individuals have target label equal to 1?",
    "text": "4. In each group, what proportion of individuals have target label equal to 1?\nWe can use groupby() and mean() to get the proportions for each racial group.\n\n# Calculate the proportion of employed individuals in each racial group\ndf.groupby(\"group\")[\"label\"].mean()\n\ngroup\n1    0.455242\n2    0.416756\n3    0.431818\n4    0.357143\n5    0.461347\n6    0.499359\n7    0.430380\n8    0.466711\n9    0.353955\nName: label, dtype: float64"
  },
  {
    "objectID": "posts/Auditing-Bias/index.html#intersectional-trends",
    "href": "posts/Auditing-Bias/index.html#intersectional-trends",
    "title": "Blog Post 2 - Auditing Bias",
    "section": "5. Intersectional Trends",
    "text": "5. Intersectional Trends\nWe can take a look at the intersection of race and sex and how different combinations of the two effect employment. In this dataset 1 is male and 2 is female.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(10,6))\nintersectional_df = df.groupby([\"group\", \"SEX\"])[\"label\"].mean().reset_index()\nsns.barplot(x=\"group\", y=\"label\", hue=\"SEX\", data=intersectional_df)\nplt.title(\"Employment Proportion by Race and Sex\")\nplt.xlabel(\"Race\")\nplt.ylabel(\"Proportion Employed\")\nplt.legend(title=\"Sex\")\nplt.show()\n\n\n\n\n\n\n\n\nThis bar chart displays the proportion of individuals employed across different racial groups, further broken down by sex. The two colors represent males (1.0) and females (2.0). Across most racial groups, there appears to be a consistent disparity in employment rates between males and females, with males generally having higher employment proportions than females. For example, in groups 4, 6, and 8, males show noticeably higher employment rates than females. However, in some cases, such as group 5, females have a higher employment rate than males. These disparities suggest that both race and sex impact employment outcomes, which could be an important factor to consider when evaluating fairness and bias in predictive models. Note that we are taking race out of our model as a feature, but this graph is interesting.\nNow that we’ve done some basic exploring of the data, we can start to build our model. I’ve decided to employ a RandomForest to try to predict employment status. To do this, I must tune the hyperparameter max_depth. In a Random Forest, tuning max_depth controls the maximum depth of each decision tree. The goal here is to try to balancing model complexity by preventing overfitting and underfitting with overly complex/simple trees.\nI created a for loop that tries different max_depths on Random Forest models. It uses 5 fold cross validation to test each max_depth and takes the mean of these 5 scores to determine which value performs the best.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\n# Define a range of max_depth values to test\nmax_depth_values = [5, 10, 20, 30]  # Testing different depths\n\n\nbest_score = 0  # Initialize best score\nbest_depth = None  # Initialize best depth\n\n# Loop through different max_depth values\nfor depth in max_depth_values:\n    print(f\"Training Random Forest with max_depth={depth}\")\n\n    # Initialize Random Forest with current max_depth\n    RF = RandomForestClassifier(max_depth=depth, random_state=7)\n\n    # Perform cross-validation (5-fold)\n    scores = cross_val_score(RF, X_train, y_train, cv=5)\n\n    # Compute mean accuracy\n    mean_score = np.mean(scores)\n    print(f\"Mean CV Accuracy: {mean_score:.4f}\\n\")\n\n    # Track the best performing max_depth\n    if mean_score &gt; best_score:\n        best_score = mean_score\n        best_depth = depth\n\n# Print best depth and score\nprint(f\"Best max_depth: {best_depth} with accuracy: {best_score:.4f}\")\n\nTraining Random Forest with max_depth=5\nMean CV Accuracy: 0.8168\n\nTraining Random Forest with max_depth=10\nMean CV Accuracy: 0.8252\n\nTraining Random Forest with max_depth=20\nMean CV Accuracy: 0.8252\n\nTraining Random Forest with max_depth=30\nMean CV Accuracy: 0.8066\n\nBest max_depth: 20 with accuracy: 0.8252\n\n\nBased on the cross-validation results, max_depth = 20 is the optimal choice for the Random Forest model. At this depth, the model achieved the highest mean CV accuracy of 0.8252, indicating that it balances predictive power and generalization. While max_depth = 10 produced the same accuracy, deeper trees (max_depth = 30 and beyond) led to a decline in accuracy, suggesting potential overfitting.\nUsing max_depth = 20 ensures that the model captures important patterns in the data without becoming too complex. This depth allows the model to generalize well to unseen data while maintaining strong performance. The final model will now be trained using this optimized hyperparameter and evaluated on the test set.\nHere, I define and fit my Random Forest model. We can also extract the model predictions and store them in y_hat.\n\nfrom sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier(max_depth=20, random_state=7)\nRF.fit(X_train, y_train)\ny_hat = RF.predict(X_test)"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Abstract\nIn this study, we apply machine learning techniques to classify three penguin species—Adelie, Chinstrap, and Gentoo—using the famous Palmer Penguins dataset. We preprocess the data by handling missing values and encoding categorical variables, then explored feature relationships through visualizations. Then, we find the three most effective features for a Random Forest classifier and trained it using selected features. We were able to achieve 99% accuracy on the test set. This underscores the effectiveness of the chosen features and model in accurately distinguishing between the penguin species.\n\n\nExplore\nBefore we start exploring the data, we need to load in the Palmer’s Penguins Dataset and take a look at the data we are given.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nNow, we need to clean this data. First, we can see that there are some missing values. In this class, we haven’t learned how to impude the missing values yet, so I will just get rid of the rows that have N/A values. Then, I will use the provided code to “one-hot ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍encode” the categorical features. These categorical columns should now be True/False rather than Yes/No.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nNow, I am going to merge the our target column with our features into a new dataset so that I can create visualizations easier.\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = X_train.copy()\ndf[\"Species\"] = y_train.flatten().astype(int)\n\n\nfig, ax = plt.subplots(figsize=(6, 5))\nsns.scatterplot(data=df, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", hue=\"Species\", style=\"Species\", ax=ax)\nax.set(title=\"Body Mass vs. Flipper Length by Species\", xlabel=\"Flipper Length (mm)\", ylabel=\"Body Mass (g)\")\nhandles, _ = ax.get_legend_handles_labels()\nax.legend(handles, [\"Adelie\", \"Chinstrap\", \"Gentoo\"], title=\"Species\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis scatterplot shows the relationship between Flipper Length (mm) and Body Mass (g) by the three penguin species: Adelie, Chinstrap, and Gentoo. My first takeaway from this plot is that Gentoo penguins (represented by squares) have significantly higher higher body mass and longer flipper lengths than the other two species. Additionally, there is notable overlap between Adelie and Chinstrap penguins (represented by circles and X’s) in the middle of the graph. For a classification problem, this is a little alarming for me because this overlap suggests that Flipper Length and Body Mass alone won’t be able to reliably distinguish between Adelie and Chinstrap species. I may need to look for a different quantatative relationship to get to 100% accuracy.\n\n# map to have species name on x-axis rather than 0,1,2\nmap = {0: \"Adelie\", 1: \"Chinstrap\", 2: \"Gentoo\"}\n\n# Replace species numbers with names\ndf[\"Species\"] = df[\"Species\"].map(map)\nfig, ax = plt.subplots(figsize=(6, 5))\nsns.boxplot(data=df, x=\"Species\", y=\"Culmen Length (mm)\", ax=ax)\nax.set(title=\"Culmen Length by Species\", xlabel=\"Species\", ylabel=\"Culmen Length (mm)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis boxplot illustrates the distribution of culmen length (mm) across the three penguin species. One observation is that Adelie penguins have the shortest culmen length by far. They have a median significantly lower than both Chinstrap and Gentoo penguins. Chinstrap penguins have the longest culmen length on average, with a higher IQR. Gentoo penguins tend to fall between the two but exhibit a slightly more compact distribution. This visualization reinforces the idea that culmen length could be a useful feature for distinguishing Adelie penguins from the other two species, but there may be some overlap between Chinstrap and Gentoo, so I’ll have to address this when I am searching for my 3 features.\n\n# Create a new 'Sex' column based on one-hot encoding to make aggregating table easier\ndf[\"Sex\"] = df[\"Sex_MALE\"].map({True: \"Male\", False: \"Female\"})\n\n\nsummary_table = df.groupby([\"Species\", \"Sex\"]).aggregate({\n    \"Culmen Length (mm)\": [\"mean\", \"std\"],\n    \"Culmen Depth (mm)\": [\"mean\", \"std\"],\n    \"Flipper Length (mm)\": [\"mean\", \"std\"],\n    \"Body Mass (g)\": [\"mean\", \"std\"]\n})\n\nsummary_table\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nSpecies\nSex\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\nFemale\n37.355769\n1.896686\n17.644231\n0.928928\n188.038462\n5.423122\n3353.365385\n264.576467\n\n\nMale\n40.451786\n2.449044\n19.064286\n1.048536\n192.839286\n6.893414\n4066.071429\n320.703367\n\n\nChinstrap\nFemale\n46.722581\n3.169933\n17.612903\n0.801558\n192.064516\n5.898788\n3523.387097\n294.953067\n\n\nMale\n51.312000\n1.633840\n19.256000\n0.779466\n200.480000\n6.325346\n4008.000000\n375.951570\n\n\nGentoo\nFemale\n45.455102\n1.971787\n14.210204\n0.536674\n212.836735\n3.466187\n4684.693878\n297.463948\n\n\nMale\n49.046512\n2.303760\n15.741860\n0.793495\n221.186047\n5.279141\n5481.976744\n302.830181\n\n\n\n\n\n\n\nThis table highlights consistent differences between male and female penguins across all three species, suggesting that sex could be a valuable feature for species classification. I noticed that male penguins tend to have larger body mass, culmen length, and flipper length compared to females within the same species. These differences are especially pronounced in Body Mass (g) and Flipper Length (mm), where males exhibit significantly higher mean values than females. Since the magnitude of these differences varies by species—for example, Gentoo males are much heavier than Chinstrap or Adelie. So maybe, sex in combination with other quantitative features, can help differentiate species. Given that some species, like Adelie and Chinstrap, show overlap in other features (like in the first graph), incorporating sex into the model may improve classification accuracy by providing an additional distinguishing factor.\n\n\nModel\nNow that I’ve done a little exploring of the data, it is time to determine which features will be predict the penguins in the “test” dataset. At a high level, I will iterate through every combination of 1 qualitative variable and 2 quantitative variables and use cross validation to determine which combination performs best within the training data.\n\nimport pandas as pd\nimport numpy as np\nfrom itertools import combinations\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nqualitative_features = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Clutch Completion_No\", \"Clutch Completion_Yes\", \"Sex_FEMALE\", \"Sex_MALE\"]\nquantitative_features = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]\n\n\nbest_features = None\nbest_score = 0\n\n# Try combinations of 1 qualitative + 2 quantitative features\nfor qual in qualitative_features:\n    for quant_combo in combinations(quantitative_features, 2):\n        selected_features = [qual] + list(quant_combo)\n\n        # Train a Random Forest\n        RF = RandomForestClassifier(random_state=7)\n        scores = cross_val_score(RF, X_train[selected_features], y_train, cv=5)\n\n        # Compute mean accuracy\n        mean_score = np.mean(scores)\n\n        # Track the best feature combination\n        if mean_score &gt; best_score:\n            best_score = mean_score\n            best_features = selected_features\n\n# Output the best feature set and corresponding accuracy\nbest_features, best_score\n\n(['Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'],\n np.float64(0.9844645550527904))\n\n\nIn this block, I am trying all combnations of 1 qualitative and 2 quantiative features. For each combo, I am creating a Random Forest object (I chose lucky number 7 for reproducability) and using the cross_val_score function to see how the model does with these features. I am familiar with Random Forests from STAT0218 but I don’t remember how the hyperparameters work (like max_depth) so I didn’t tune those. I then took the vector of 5 cross-validation scores to gauge how each combination performed. I then updated the best score and selected features if they were the best so far. I found that the best three performing features were [‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Culmen Depth (mm)’] which had an average accuracy of about 98% when I cross-validated. This is pretty in line with some of my findings from the explore phase.\n\n\nTesting\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)','Sex_MALE']\n\nRF = RandomForestClassifier(random_state=7)\nRF.fit(X_train[cols], y_train)\ntest_score = RF.score(X_test[cols], y_test)\ntest_score\n\n0.9852941176470589\n\n\nHere, I fit a random forest using the training data and evaluation it on the unseen test data. I was able to get ~99% accuracy. Now, let’s take a look at the decision boundaries.\n\nfrom matplotlib.patches import Patch\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)','Sex_MALE', \"Sex_FEMALE\"]\nRF.fit(X_train[cols], y_train)\n\nplot_regions(RF, X_train[cols], y_train)\n\n\n\n\n\n\n\n\nI used the plot_regions() function that we were given to show how the training data looks with a Random Forest Model and the feaures Culmen Length, Culmen Depth, and Sex. The boundaries for a Random Forest are a little funky but we can see how effective this model and these features are for classifying the penguins. The blue (Gentoo), red (Adelie), and green (Chinstrap) regions show how the model segments the graph. I notice that Gentoo are in the upper left showing that culmen depth might be the primary classfying in distinguishing. For Chinstrap and Adelie there is slightly more overlap. Some Chinstrap points appear in the red region. This suggests that while culmen depth helps differentiate Gentoo penguins, it may not be as reliable in distinguishing Adelie and Chinstrap.\nThe decision boundaries for males and females have the same general shape, but are different. This indicates that sex influences the classification. The boundaries for Adelie and Chinstrap penguins appear more distinct in females than in males, which may imply that culmen depth differences between these two species are more pronounced in female penguins. This reinforces why “Sex_MALE” was selected as an important feature—it helps refine the classification process and improves separation between species.\n\nplot_regions(RF, X_test[cols], y_test)\n\n\n\n\n\n\n\n\nI used the plot_regions() function that we were given to show how the decision boundaries on the testing data with my RF model using the feaures Culmen Length, Culmen Depth, and Sex. This model achieved 67/68 accuracy on the test data, meaning that every penguin was correctly except for 1. You can see the one error in my model on the test data where the Chinstrap penguin is in the Adelie section.\n\nfrom sklearn.metrics import confusion_matrix\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)','Sex_MALE']\nRF = RandomForestClassifier(random_state=7)\nRF.fit(X_train[cols], y_train)\n\ny_test_pred = RF.predict(X_test[cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 10,  1],\n       [ 0,  0, 26]])\n\n\nThe diagonal entires are the ones my model got correctly. The confusion matrix shows that the model performed well, correctly classifying 67 out of 68 test samples. The only misclassification occurred in the second row, where one Chinstrap penguin was mistakenly classified as Adelie. This suggests that while the model is effective, there may be some overlap in feature distributions between Chinstrap and Adelie, likely due to similarities in culmen depth or length.\n\n\nDiscussion\nThe successful classification of penguin species with perfect accuracy highlights the effectiveness of the Random Forest model and the feautres that were selected during this blog post. Notably, the inclusion of Sex as a feature proved crucial, as preliminary visualizations revealed overlapping measurements between Adelie and Chinstrap species in different features. Incorporating ‘Sex’ helped resolve this overlap, leading to clearer decision boundaries. This study demonstrates that through a clear classification pipeline, machine learning can effectively address classification problems in the real world."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jackson Hanson CSCI 0451 Blog",
    "section": "",
    "text": "Replication Study\n\n\n\n\n\nBlog Post 3\n\n\n\n\n\nMar 10, 2025\n\n\nJackson Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 2 - Auditing Bias\n\n\n\n\n\nBlog Post 2!\n\n\n\n\n\nMar 4, 2025\n\n\nJackson Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nUse ML techniques to classify Palmer Penguins\n\n\n\n\n\nFeb 24, 2025\n\n\nJackson Hanson\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]